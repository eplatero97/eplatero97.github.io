```
layer: splash
title: "Dropout Layer From Scratch with PyTorch"
author: Erick Platero
category: DL
tags: [dl] 
user_profile: true 
```

During training, the `nn.Dropout()` layer is a: 

* linear operation 
  * First, it multiplies each element of its input by either 1 or 0
  * Then, it scales its outputs by a factor dependent on its user-defined Bernoulli-distribution.
* stochastic operation
  * 1 or 0 input multipliers are randomly generated by a user-defined Bernoulli-distribution.

During inference, it just computes an identity function, which is equivalent to computing a Bernoulli distribution with success probability $p$ equal to one.

This layer is used as a regularization method to mitigate over-fitting of data. It is a simple yet effective method that was presented on the [Improving neural networks by preventing co-adaptation of feature detectors]([[1207.0580\] Improving neural networks by preventing co-adaptation of feature detectors (arxiv.org)](https://arxiv.org/abs/1207.0580)) paper.

We can represent the forward operation as follows:
$$
a(Dropout(\vec{x})) =a(\vec{x} * \vec{b})
$$
where: 

* $\vec{x} \in R^n$, 
* $\vec{b} \sim Bernoulli(p)^n$,
* $a = \frac{1}{1-p}$  

> NOTE: the $n$ in $Bernoulli(p)^n$ means that we will "run" the Bernoulli function $n$ times. Further, we are representing the input as a vector but it can be a tensor of any dimension. Lastly, the variable $a$ represents a constant. 

Then, the derivative becomes:
$$
\frac{d}{d\vec{x}}a(Dropout(\vec{x})) = a*\vec{b}
$$
**Intuition?**

The intuition of this operation is that we want the network to learn different mapping configurations of the same input to reach a desired ground-truth output.  This is because we are training the model to become "Bernoulli-invariant" to some degree at the specific layer location. This adds difficulty in learning I/O mappings and thus, benefits from generalizing our data. 

**Scaling?**

The scaling factor $a$ is implemented so that the expected output of the forward pass during training is roughly equal to that during inference. The authors chose $p = .5$  and as such, the expected the expected output of the operation during training is expected to be lower than that during inference because we are dropping some elements to zero. 
$$
E[\vec{b} \sim Bernoulli(p=.5)^n] = p \approx .5 < 1 = E[\vec{b} \sim Bernoulli(1)^n]
$$
However, when we scale the output by $a = \frac{1}{1-.5} = \frac{1}{.5} =  2$ during training, we get below:
$$
E[a *\vec{b} \sim Bernoulli(p)^n] = ap \approx 2\frac{1}{2} = 1 = 1 = E[\vec{b} \sim Bernoulli(1)^n]
$$
We want to keep the mean of the distributions during training to be roughly equal to that of inference so that during inference, we will be able to harness the learned representations created during training. If not, then inputs of a different scale may effectively undermine network performance. 

However, the above formulation only evens out the expected outputs when $p = .5$. What about when we want something different such as $p = .1$? The PyTorch implementation always sets $a = \frac{1}{1-p}$, and thus, the expected outputs between training and inference will always be different for the same input if $p \not = .5$. 

To remedy this, we will set $a = \frac{1}{p}$ during the forward pass. 

> NOTE: implementation of the theory is found at the bottom of this document

Let us now analyze the code implementation to better understand the dropout layer.

**Forward/Backward pass implementation?**

```python
import torch
from typing import Tuple

class Dropout_Layer(torch.autograd.Function):
    """
    :obj: implement forward/backward pass of dropout layer
    """
    @staticmethod
    def forward(ctx, input: torch.FloatTensor, p: float) -> torch.FloatTensor:
        """
        :obj: implement Dropout layer forward pass 
        :param input: input matrix; shape: (batch_size, *feats) (asterik represents any number of feature dimensions)
        :param p: probability of each elemen in `input` to be "dropped" to zero
        """
        # calculate binary stochastic weights
        drop_probs: torch.FloatTensor = torch.tensor(p).repeat(input.shape) # shape: (batch_size, *feats)
        is_dropped: torch.BoolTensor = torch.bernoulli(drop_probs).bool() # shape: (batch_size, *feats)

        # save variables needed for backward pass 
        ctx.save_for_backward(is_dropped) 
        ctx.p = p 

        # execute dropout layer
        output = input.clone() # `output` is created so that we can perform dropout "out-place"
        output[is_dropped] = 0 # "drop" (zero-out) all inputs corresponding to a `True` even in `is_dropped` 
        output /= p if p > 0 else 1 # scale output by p if its greater than 0, else perform identity division
        return output # shape: (batch_size, *feats)
        
    @staticmethod
    def backward(ctx, incoming_grad: torch.FloatTensor) -> Tuple[torch.FloatTensor,None]:
        """
        :obj: implement Dropout layer backward pass
        :param incoming_grad: gradient of loss w.r.t. `output` from forward pass; shape: (batch_size, feats) 
        :return: gradient of Loss w.r.t. `input` from forward pass  
        """

        # unpack saved variables during forward pass
        is_dropped, = ctx.saved_tensors
        p = ctx.p

        # calculate gradient
        out_grad = incoming_grad.clone()
        out_grad[is_dropped] = 0
        out_grad /= p if p > 0 else 1 # shape: (batch_size, *feats)
        return out_grad, None 
```



**Implement dropout layer API:**

```python
class Dropout(nn.Module):
    """
    :obj: implement linear layer API 
    """
    def __init__(ctx, p: float = .5):
        super().__init__()
        ctx.p = p # probability of each elemen in `input` to be "dropped" to zero
        
    def forward(ctx, input: torch.FloatTensor) -> torch.FloatTensor:
        if not ctx.training:
            output = input
        else:
            output = Dropout_Layer.apply(input, ctx.p)
        
        return output
```



**Assert results match PyTorch's official implementation:**

```python
# pytorch dropout implementation
x = torch.arange(1, 6).view(1, -1).float()
x.requires_grad = True
pt_drop = nn.Dropout(p = .5)
out1 = pt_drop(x) # tensor([[ 2.,  0.,  0.,  8., 10.]], grad_fn=<MulBackward0>)
out2 = out1.sum().view(1,1)
out2.backward()
x.grad # tensor([[2., 0., 0., 2., 2.]])

# manual dropout implementation 
x = torch.arange(1, 6).view(1, -1).float()
x.requires_grad = True
man_drop = Dropout(p = .5)
out1 = man_drop(x) # tensor([[2., 0., 0., 8., 0.]], grad_fn=<Dropout_LayerBackward>)
out2 = out1.sum().view(1,1)
out2.backward()
x.grad # tensor([[2., 0., 0., 2., 0.]])
```

> NOTE: due to their stochastic nature, your pytorch and manual implementation will most likely differ. However, you should see a resemblance in the I/O between the two implementations. 

**Scaling Dropout Layer when p = .5**

```python
# identity function during inference
p = 1.
x = torch.tensor(p).repeat(10)
trials = [torch.bernoulli(x).mean().item() for i in range(100)]
torch.as_tensor(trials).mean() # 1.

# dropout effect during training without scaling 
p = 1/2
x = torch.tensor(p).repeat(10)
trials = [torch.bernoulli(x).mean().item() for i in range(100)]
torch.as_tensor(trials).mean() # around .5

# dropout effect during training with scaling
p = 1/2
x = torch.tensor(p).repeat(10)
a = 1 / (1-p) # 2
trials = [torch.bernoulli(x).mean().item() * a for i in range(100)]
torch.as_tensor(trials).mean() # around 1.
```

**Scaling Dropout Layer when p = .1 with pytorch implementation**

```python
# identity function during inference
p = 1.
x = torch.tensor(p).repeat(10)
trials = [torch.bernoulli(x).mean().item() for i in range(100)]
torch.as_tensor(trials).mean() # 1.

# dropout effect during training without scaling 
p = 1/10
x = torch.tensor(p).repeat(10)
trials = [torch.bernoulli(x).mean().item() for i in range(100)]
torch.as_tensor(trials).mean() # around .1

# dropout effect during training with scaling
p = 1/10
x = torch.tensor(p).repeat(10)
a = 1 / (1-p) # 1.11
trials = [torch.bernoulli(x).mean().item() * a for i in range(100)]
torch.as_tensor(trials).mean() # around .11
```

**Scaling Dropout Layer when p = .1 with manual implementation**

```python
# identity function during inference
p = 1.
x = torch.tensor(p).repeat(10)
trials = [torch.bernoulli(x).mean().item() for i in range(100)]
torch.as_tensor(trials).mean() # 1.

# dropout effect during training without scaling 
p = 1/10
x = torch.tensor(p).repeat(10)
trials = [torch.bernoulli(x).mean().item() for i in range(100)]
torch.as_tensor(trials).mean() # around .1

# dropout effect during training with scaling
p = 1/10
x = torch.tensor(p).repeat(10)
a = 1 / p # 10
trials = [torch.bernoulli(x).mean().item() * a for i in range(100)]
torch.as_tensor(trials).mean() # around 1.
```


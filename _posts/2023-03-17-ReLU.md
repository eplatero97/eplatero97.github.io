---
layout: single
classes: wide
title: "ReLU"
author: Erick Platero
category: DL
tags: [dl]

---
# Activation Functions

In D.L., our **objective** is, almost always, to find a **set of weights** that **minimizes error.** All of these sets of weights are **linear operations** and hence, if performed alone, we would attain just a simple **multiple linear regression model.** 

##### What’s the Problem with Linear Models?

If inputs are left untouched, they are not **flexible** as they can only model linear relationships while most data out there has a **non-linear patterns.** Hence, we need to find a way to force our model to be able to **learn non-linear patterns.** 

##### How do we do this?

After a set of linear operations, we apply to the new **input** created by the linear operations ($Ax = \hat{y}$) a **non-linear activation function**.

Suppose we have a simple linear model $\hat{y}=ax+b$. These $\hat{y}$ form a linear operation such as below:



Well, given our orange line ($\hat{y}$), we then apply a **non-linear activation function** so as to **transform** our linear model into a **fixed non-linear model** such as below:

<img src="https://www.researchgate.net/profile/Hoon_Chung2/publication/309775740/figure/fig1/AS:538049215381504@1505292337270/The-most-common-nonlinear-activation-functions.png" alt="The most common nonlinear activation functions. | Download ..." style="zoom: 50%;" />

**Why is this a fixed non-linear operation?**

Because whatever formula we use for our non-linear operation, **we do NOT** have a set of weights on it that try to learn an optimal non-linear representation. It will always follow a **fixed, single transformation.**

##### Well, isn’t our purpose to find an optimal non-linear operation?

Yes and no. We find an optimal non-linear operation by letting our set of **linear weights** learn a **representation of the data** that, **once fed to the non-linear operation**, will **correctly identify the new pattern.** Hence, the objective of our linear weights now becomes to **find a representation of the data that, once fed to the non-linear activation, will correctly learn the non-linear patterns.**

##### How do we backpropagate these non-linear activations?

Since non-linear activations are non-linear, we can't directly use the input as the gradient as we do with linear operations. To backpropagate, we need **two things**:

1. The **input ($\hat{y}$) that was fed to the non-linear activation**
2. The **derivative equation of the non-linear function**


Given that we want to apply the non-linear operation to every input, we can classify these operations as element-wise. 

This has important implications for how we can calculate our gradient.

**First**, as we learned in the "Linear Layer" tutorial, the dimension of the incoming gradient from our subsequent operation equals the dimension of the output from our non-linear operation.

Now, since the output of the non-linear operations equals the dimension of the input, we are able to calculate the corresponding chain-gradient with a simple Hadamard product (element-wise multiplication) between our incoming gradient and our current non-linear operation. In other words,

```input.shape == output.shape == incoming_grad.shape```

**Second**, given that there are no weight parameters to these operations, it holds two implications:

i) from a backward perspective, these operations are only intermediate variables, and 

ii) we can apply the derivative of the equation to each input, as shown below:

$$z = \sigma(y)=\sigma(x_ow_0+x_1w_1+x_2w_2+x_3w_3) = \sigma(x_ow_0)+\sigma(x_1w_1)+\sigma(x_2w_2)+\sigma(x_3w_3)$$

Hence,

$$\frac{\partial z}{\partial y} = \frac{\partial z}{\partial y}(x_ow_0+x_1w_1+x_2w_2+x_3w_3) = \frac{\partial z}{\partial y}(x_ow_0)+\frac{\partial z}{\partial y}(x_1w_1)+\frac{\partial z}{\partial y}(x_2w_2)+\frac{\partial z}{\partial y}(x_3w_3)$$

Now that we generally understand how to implement non-linear operations, it begs to ask, **what are some common non-linear operations?**

Some common activation functions are shown below:

* ReLU
* Hyperbolic Tangent (tanh)
* Leaky ReLU

Each has their own unique properties and usually, finding the best best corresponding non-linear activation to a model is left to trial and error.

# ReLU
In this tutorial, we will first focus on implementing the ReLU layer and towards the end, for comparison purposes, we will define alternative activation functions.

ReLU is a piece-wise linear, vector-valued function that introduces non-linearity into our model. Its simplicity has had a significant impact on the deep learning community.

The ReLU's forward and backward passes can be viewed as "gates" that control the flow of operations. 

During the forward pass, ReLU retains the input value if it's greater than zero; otherwise, it sets the input to zero.

```python
torch.randn((2,2)).cuda()  # tensor([[ 1.6169, -0.8602],
                           #         [ 0.2214, -0.4084]], device='cuda:0')
```

For the inputs that were "cut" to zero, its gradients are turned to zero while the rest of the values become 1. Hence, and given that ReLU is an intermediate operation, ReLU either restricts values of the incoming gradients or lets them "flow". This process is graphed below:

![image.png]({{ site.baseurl }}/assets/relu/image.png)

Such simple conditions make ReLU a "lightweight" operation as it does not take much to compute its forward and backward method

Such properties, and its surprising effectiveness to model non-linearity, have made ReLU a very popular choice of option for most DL architectures.

Let us model this process in PyTorch


```python
import torch
import torch.nn as nn
# custom ReLU function 
# Remember that:
# input.shape == out.shape == incoming_gradient.shape

class ReLU_layer(torch.autograd.Function):
    
    @staticmethod
    def forward(self, input):
        # save input for backward() pass 
        self.save_for_backward(input) # wraps in a tuple structure
        activated_input = torch.clamp(input, min = 0)
        return activated_input

    @staticmethod
    def backward(self, incoming_grad):
        """
        In the backward pass we receive a Tensor containing the 
        gradient of the loss with respect to our f(x) output, 
        and we now need to compute the gradient of the loss
        wrt the input.
        """
        # keep in mind that the gradient of ReLU is binary = {0,1}
        # hence, we will either keep the element of the output_grad_wrt_loss
        # or turn it to zero
        input, = self.saved_tensors
        output_grad = incoming_grad.clone()
        output_grad[input < 0] = 0
        return output_grad 
```


```python
# Wrap ReLU_layer function in nn.module
class ReLU(nn.Module):
    def __init__(self):
        super().__init__()

        
    def forward(self, input):
        output = ReLU_layer.apply(input)
        return output
    
```


```python
# test function with linear + relu layer
dummy_input= torch.ones((1,2)) # input 

# forward pass
linear = nn.Linear(2,3)
relu = ReLU()
linear2 = nn.Linear(3,1)

output1 = linear(dummy_input)
output2 = relu(output1)
output3 = linear2(output2)
output3 # tensor([[0.2316]], grad_fn=<AddmmBackward>)
```

```python
# backward pass
output3.backward()
```


```python
# check computed gradients of 1st linear layaer
list(linear.parameters())[0].grad # tensor([[0.1558, 0.1558],
                                  #        [0.0000, 0.0000],
                                  #        [0.0000, 0.0000]]
```



# MNIST

Now that we have validated our operation, let's us apply ReLU to the MNIST dataset by building a standard neural network with the following linear parameters:

```[128, 64, 10]```


```python
class NeuralNet(nn.Module):
    def __init__(self, num_units = 128, activation = ReLU()):
        super().__init__()
        
        # fully-connected layers
        self.fc1 = nn.Linear(784,num_units)
        self.fc2 = nn.Linear(num_units , num_units//2)
        self.fc3 = nn.Linear(num_units // 2, 10)
        
        # init activation
        self.activation = activation
        
    def forward(self,x):
        
        # 1st layer
        output = self.activation(self.fc1(x))
        
        # 2nd layer
        output = self.activation(self.fc2(output))
        
        # 3rd layer
        output = self.fc3(output)
        
        # output.shape = (B, 10)
        return output
        
```


```python
# instantiate model and feed it to GPU
device = torch.device('cuda')
model = NeuralNet().to(device)
model # NeuralNet(
      #      (fc1): Linear(in_features=784, out_features=128, bias=True)
      #      (fc2): Linear(in_features=128, out_features=64, bias=True)
      #      (fc3): Linear(in_features=64, out_features=10, bias=True)
      #      (activation): ReLU()
      #    )
```




```python
# define optimizer
from torch import optim
optimizer = optim.SGD(model.parameters(), lr = .01)
```


```python
# define criterion
criterion = nn.CrossEntropyLoss()
```


```python
# import training MNIST dataset
import torchvision
from torchvision import transforms
import numpy as np
from torch.utils.data import DataLoader
from torchvision.utils import make_grid 
import matplotlib.pyplot as plt
plt.style.use('ggplot')

root = r'C:\Users\erick\PycharmProjects\untitled\3D_2D_GAN\MNIST_experimentation'
train_mnist = torchvision.datasets.MNIST(root = root, 
                                      train = True, 
                                        transform = transforms.ToTensor(),
                                      download = False, 
                                  )

train_mnist.data.shape # torch.Size([60000, 28, 28])
```



```python
# import evaluation MNIST dataset

eval_mnist = torchvision.datasets.MNIST(root = root, 
                                      train = False,
                                      transform = transforms.ToTensor(),
                                      download = False, 
                                  )
eval_mnist.data.shape # torch.Size([10000, 28, 28])
```



```python
# visualize data
# visualize our data

grid_images = np.transpose(make_grid(train_mnist.data[:64].unsqueeze(1)), (1,2,0))
plt.figure(figsize=(8,8))
plt.axis("off")
plt.title("Training Images")
plt.imshow(grid_images,cmap = 'gray')
```

![png]({{ site.baseurl }}/assets/relu/ReLU_16_1.png)
    



```python
# normalize data
train_mnist.data = (train_mnist.data.float() - train_mnist.data.float().mean()) / train_mnist.data.float().std()
eval_mnist.data = (eval_mnist.data.float() - eval_mnist.data.float().mean()) / eval_mnist.data.float().std()
```


```python
# parse data to batches of 128

# pin_memory = True if you have CUDA. It will speed up I/O

train_dl = DataLoader(train_mnist, batch_size = 64, 
                      shuffle = True, pin_memory = True)

eval_dl = DataLoader(eval_mnist, batch_size = 128, 
                      shuffle = True, pin_memory = True)


batch_images, batch_labels = next(iter(train_dl))
print(f"batch_images.shape: {batch_images.shape}")
print('-'*50)
print(f"batch_labels.shape: {batch_labels.shape}")
```

    batch_images.shape: torch.Size([64, 1, 28, 28])
    --------------------------------------------------
    batch_labels.shape: torch.Size([64])


# Train Neural Net


```python
# compute average accuracy of batch

def accuracy(pred, labels):
    # predictions.shape = (B, 10)
    # labels.shape = (B)
    
    n_batch = labels.shape[0]
    
    # extract idx of max value from our batch predictions
    # predicted.shape = (B)
    _, preds = torch.max(pred, 1)
    
    
    # compute average accuracy of our batch
    compare = (preds == labels).sum()
    return compare.item() / n_batch
    
    
```


```python

def train(model, iterator, optimizer, criterion):
    
    # hold avg loss and acc sum of all batches
    epoch_loss = 0
    epoch_acc = 0
    
    
    for batch in iterator:
        
        # zero-out all gradients (if any) from our model parameters
        model.zero_grad()
        
        
        # extract input and label
        
        # input.shape = (B, 784), "flatten" image
        input = batch[0].view(-1,784).cuda().float() # shape: (B, 784), "flatten" image
        # label.shape = (B)
        label = batch[1].cuda()
        
        
        # Start PyTorch's Dynamic Graph
        
        # predictions.shape = (B, 10)
        predictions = model(input)
        
        # average batch loss 
        loss = criterion(predictions, label)
        
        # calculate grad(loss) / grad(parameters)
        # "clears" PyTorch's dynamic graph
        loss.backward()
        
        
        # perform SGD "step" operation
        optimizer.step()
        
        
        # Given that PyTorch variables are "contagious" (they record all operations)
        # we need to ".detach()" to stop them from recording any performance
        # statistics
        
        
        # average batch accuracy
        acc = accuracy(predictions.detach(), label)
        
        # record our stats
        epoch_loss += loss.detach()
        epoch_acc += acc
        
    # NOTE: tense.item() unpacks Tensor item to a regular python object 
    # tense.tensor([1]).item() == 1
        
    # return average loss and acc of epoch
    return epoch_loss.item() / len(iterator), epoch_acc / len(iterator)

```


```python
def evaluate(model, iterator, criterion):
    
    epoch_loss = 0
    epoch_acc = 0
        
    # turn off grad tracking as we are only evaluation performance
    with torch.no_grad():
    
        for batch in iterator:

            # extract input and label       
            input = batch[0].view(-1,784).cuda()
            label = batch[1].cuda()


            # predictions.shape = (B, 10)
            predictions = model(input)

            # average batch loss 
            loss = criterion(predictions, label)

            # average batch accuracy
            acc = accuracy(predictions, label)

            epoch_loss += loss
            epoch_acc += acc
        
    return epoch_loss.item() / len(iterator), epoch_acc / len(iterator)
```


```python
import time

# record time it takes to train and evaluate an epoch
def epoch_time(start_time, end_time):
    elapsed_time = end_time - start_time # total time
    elapsed_mins = int(elapsed_time / 60) # minutes
    elapsed_secs = int(elapsed_time - (elapsed_mins * 60)) # seconds
    return elapsed_mins, elapsed_secs
```


```python
N_EPOCHS = 25

# track statistics
track_stats = {'activation': [],
               'epoch': [],
               'train_loss': [],
              'train_acc': [],
              'valid_loss':[],
              'valid_acc':[]}


best_valid_loss = float('inf')

for epoch in range(N_EPOCHS):

    start_time = time.time()
    
    train_loss, train_acc = train(model, train_dl, optimizer, criterion)
    valid_loss, valid_acc = evaluate(model, eval_dl, criterion)
    
    end_time = time.time()
    
    # record operations
    track_stats['activation'].append('ReLU')
    track_stats['epoch'].append(epoch + 1)
    track_stats['train_loss'].append(train_loss)
    track_stats['train_acc'].append(train_acc)
    track_stats['valid_loss'].append(valid_loss)
    track_stats['valid_acc'].append(valid_acc)
    
    

    epoch_mins, epoch_secs = epoch_time(start_time, end_time)
    
    # if this was our best performance, record model parameters
    if valid_loss < best_valid_loss:
        best_valid_loss = valid_loss
        torch.save(model.state_dict(), 'best_linear_relu_params.pt')
    
    # print out stats
    print('-'*75)
    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')
    print(f'\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')
    print(f'\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')

```

    ---------------------------------------------------------------------------
    Epoch: 01 | Epoch Time: 0m 12s
    	Train Loss: 2.257 | Train Acc: 19.67%
    	 Val. Loss: 2.208 |  Val. Acc: 11.47%
    ---------------------------------------------------------------------------
    Epoch: 02 | Epoch Time: 0m 15s
    	Train Loss: 1.928 | Train Acc: 35.99%
    	 Val. Loss: 2.658 |  Val. Acc: 15.09%
    ---------------------------------------------------------------------------
    Epoch: 03 | Epoch Time: 0m 17s
    	Train Loss: 1.684 | Train Acc: 45.90%
    	 Val. Loss: 2.697 |  Val. Acc: 18.07%
    ---------------------------------------------------------------------------
    Epoch: 04 | Epoch Time: 0m 17s
    	Train Loss: 1.530 | Train Acc: 50.23%
    	 Val. Loss: 3.533 |  Val. Acc: 16.05%
    ---------------------------------------------------------------------------
    Epoch: 05 | Epoch Time: 0m 16s
    	Train Loss: 1.444 | Train Acc: 51.98%
    	 Val. Loss: 3.861 |  Val. Acc: 16.42%
    ---------------------------------------------------------------------------
    Epoch: 06 | Epoch Time: 0m 17s
    	Train Loss: 1.393 | Train Acc: 52.64%
    	 Val. Loss: 4.699 |  Val. Acc: 16.97%
    ---------------------------------------------------------------------------
    Epoch: 07 | Epoch Time: 0m 17s
    	Train Loss: 1.359 | Train Acc: 53.11%
    	 Val. Loss: 5.268 |  Val. Acc: 16.34%
    ---------------------------------------------------------------------------
    Epoch: 08 | Epoch Time: 0m 16s
    	Train Loss: 1.335 | Train Acc: 53.38%
    	 Val. Loss: 4.952 |  Val. Acc: 14.97%
    ---------------------------------------------------------------------------
    Epoch: 09 | Epoch Time: 0m 17s
    	Train Loss: 1.319 | Train Acc: 53.84%
    	 Val. Loss: 5.703 |  Val. Acc: 15.13%
    ---------------------------------------------------------------------------
    Epoch: 10 | Epoch Time: 0m 17s
    	Train Loss: 1.304 | Train Acc: 54.26%
    	 Val. Loss: 5.436 |  Val. Acc: 15.64%
    ---------------------------------------------------------------------------
    Epoch: 11 | Epoch Time: 0m 17s
    	Train Loss: 1.292 | Train Acc: 54.25%
    	 Val. Loss: 5.915 |  Val. Acc: 15.70%
    ---------------------------------------------------------------------------
    Epoch: 12 | Epoch Time: 0m 17s
    	Train Loss: 1.279 | Train Acc: 54.77%
    	 Val. Loss: 5.348 |  Val. Acc: 17.54%
    ---------------------------------------------------------------------------
    Epoch: 13 | Epoch Time: 0m 18s
    	Train Loss: 1.269 | Train Acc: 55.03%
    	 Val. Loss: 5.428 |  Val. Acc: 14.49%
    ---------------------------------------------------------------------------
    Epoch: 14 | Epoch Time: 0m 17s
    	Train Loss: 1.258 | Train Acc: 55.50%
    	 Val. Loss: 5.109 |  Val. Acc: 15.97%
    ---------------------------------------------------------------------------
    Epoch: 15 | Epoch Time: 0m 18s
    	Train Loss: 1.250 | Train Acc: 55.75%
    	 Val. Loss: 5.109 |  Val. Acc: 15.11%
    ---------------------------------------------------------------------------
    Epoch: 16 | Epoch Time: 0m 18s
    	Train Loss: 1.240 | Train Acc: 56.00%
    	 Val. Loss: 5.531 |  Val. Acc: 15.63%
    ---------------------------------------------------------------------------
    Epoch: 17 | Epoch Time: 0m 18s
    	Train Loss: 1.232 | Train Acc: 56.31%
    	 Val. Loss: 5.849 |  Val. Acc: 14.88%
    ---------------------------------------------------------------------------
    Epoch: 18 | Epoch Time: 0m 18s
    	Train Loss: 1.225 | Train Acc: 56.48%
    	 Val. Loss: 5.722 |  Val. Acc: 16.13%
    ---------------------------------------------------------------------------
    Epoch: 19 | Epoch Time: 0m 18s
    	Train Loss: 1.218 | Train Acc: 56.77%
    	 Val. Loss: 6.119 |  Val. Acc: 17.38%
    ---------------------------------------------------------------------------
    Epoch: 20 | Epoch Time: 0m 20s
    	Train Loss: 1.211 | Train Acc: 57.05%
    	 Val. Loss: 5.578 |  Val. Acc: 17.43%
    ---------------------------------------------------------------------------
    Epoch: 21 | Epoch Time: 0m 18s
    	Train Loss: 1.205 | Train Acc: 57.24%
    	 Val. Loss: 5.528 |  Val. Acc: 15.08%
    ---------------------------------------------------------------------------
    Epoch: 22 | Epoch Time: 0m 18s
    	Train Loss: 1.198 | Train Acc: 57.58%
    	 Val. Loss: 5.489 |  Val. Acc: 17.37%
    ---------------------------------------------------------------------------
    Epoch: 23 | Epoch Time: 0m 18s
    	Train Loss: 1.193 | Train Acc: 57.58%
    	 Val. Loss: 5.657 |  Val. Acc: 16.93%
    ---------------------------------------------------------------------------
    Epoch: 24 | Epoch Time: 0m 18s
    	Train Loss: 1.191 | Train Acc: 57.96%
    	 Val. Loss: 5.609 |  Val. Acc: 16.39%
    ---------------------------------------------------------------------------
    Epoch: 25 | Epoch Time: 0m 18s
    	Train Loss: 1.182 | Train Acc: 58.17%
    	 Val. Loss: 5.460 |  Val. Acc: 15.82%


# Visualization

From the above, we can tell that our model is severely suffering from overfitting by the gap between training and validation accuracy. However, to attain a better understanding, we will graph our recorded statistics and use HiPlot, a new graphing library by Facebook, to understand the overall patterns of our model

**NOTE**: If you do not have HiPlot installed, go to their [github repo](https://github.com/facebookresearch/hiplot) to find latest installation


```python
# save statistics
# track_stats = torch.load('ReLU_stats.pt')
#torch.save(track_stats, 'ReLU_stats.pt')
```


```python
# format data 
import pandas as pd

stats = pd.DataFrame(track_stats)
stats
```


Compare training vs validation statistics


```python
fig, axes = plt.subplots(nrows=1, ncols = 2,figsize = (12,4))
stats[['epoch','train_loss','valid_loss']].plot(x = 'epoch',ax=axes[0])
axes[0].title.set_text('Training and Validation Loss')
axes[0].set_ylabel('Loss')
stats[['epoch','train_acc','valid_acc']].plot(x = 'epoch',ax = axes[1])
axes[1].title.set_text('Training and Validation Accuracy')
axes[1].set_ylabel('Accuracy')
plt.tight_layout()
plt.legend(loc = 'upper left')
plt.show()
```

    
![image.png]({{ site.baseurl }}/assets/relu/ReLU_29_1.png)
    


From the above, we can confidently conclude that our model is severely suffering from overfitting due to the large gap in loss and accuracy statistics.

Now, let us use HiPlot to attain a more complete picture of our model's performance


```python
# organize data to hiplot format
data = []
for row in stats.iterrows():
    data.append(row[1].to_dict())
# plot graph
import hiplot as hip
hip.Experiment.from_iterable(data).display(force_full_width = False)
```

<iframe src="/assets/html/relu_hiplot_graph.html" width="100%" height="500px"></iframe>


There are numerous insights that we can gather from the above graph:

* As number of epochs increase, train loss correspondingly decreases while training accuracy increases
* However, a higher training accuracy corresponded with a high validation loss, which means that on such instances, our model's prediction on the validation set was far from the truth. 
* Further, we reached our best validation accuracy of 18% on epoch 3! This gives us insight that our model was very quick to overfit the data and hence, a lower learning rate plus an added regularization term might produce better results.

With this knowledge, we can make more informed decision on how we can "tweak" our architecture to possibly produce better results. We will perform these tweaks on our [lr and regularization]() tutorial.

# Comparison

Now, we will compare the performance of our ReLU activation function with two other activation functions:

* Tanh and
* Leaky ReLU

As was previously alluded, distinct activation functions inherit distinct properties.

ReLU has "gate-like" properties but suffers from the "dying ReLU" problem, where negative inputs and their gradients become zero, preventing weight updates.

Tanh also has issues, specifically the "vanishing gradient" problem. Its function and derivative are given by:

$$
f(x)=\frac{e^x - e^{-x}}{e^x + e^{-x}} \quad f'(x) = 1 - f(x)^2
$$

![Hyper-parameters in Action! Part I — Activation Functions](https://miro.medium.com/max/848/1*SeCBB7lfA7KPJ-T1Mi7GRg.png)

As inputs move away from zero, the gradient approaches zero, reducing the ability to make significant weight changes.

Leaky ReLU addresses the "dying ReLU" problem by allowing negative inputs to have a non-zero gradient. Its function and derivative are:

$$
f(x) = \begin{cases} 0.01x & x < 0 \\ x & x \geq 0 \end{cases} \quad f'(x) = \begin{cases} 0.01 & x < 0 \\ 1 & x \geq 0 \end{cases}
$$

![Activation Functions | Code Odysseys](https://codeodysseys.com/posts/activation-functions/graph-activation-6.png)

Negative inputs are scaled by a small factor (usually 0.01) instead of being set to zero, allowing for potential recovery and meaningful weight updates.

Figuring the optimal activation function for a model is often a matter of trial and error. Hence, we will apply the equivalent architecture as we did above but this time, we will use our newly defined activation functions.


```python
################## Tanh Layer #######################

import torch.nn as nn
import torch

class tanh_layer(torch.autograd.Function):
    
    def __init__(self):
        ''
    
    def tanh(self,x):
        tanh = (x.exp() - (-x).exp()) / (x.exp() + (-x).exp())
        return tanh
    
    # forward pass
    def forward(self, input):
        # save input for backward() pass 
        self.save_for_backward(input) 
        activated_input = self.tanh(input)
        return activated_input

    # integrate backward pass with incoming_grad
    def backward(self, incoming_grad):

        input, = self.saved_tensors
        chained_grad = (1 - self.tanh(input)^2)  * incoming_grad
        return chained_grad
```


```python
class Tanh(nn.Module):
    def __init__(self):
        super().__init__()
        self.tanh = tanh_layer()
        
    def forward(self, input):
        output = self.tanh.forward(input)
        return output
    
```


```python
################## Leaky ReLU Layer #######################

class Leaky_ReLU_layer(torch.autograd.Function):
    
    @staticmethod
    def forward(self, input):
        # making operation in-place to tensors that hold "history" will mess up
        # the dynamic graph. Thus, clone it.
        activated_input = input.clone()
        self.save_for_backward(activated_input)
        activated_input[activated_input < 0] *= .01
        return activated_input

    @staticmethod
    def backward(self, incoming_grad):

        input, = self.saved_tensors
        output_grad = incoming_grad.clone()
        output_grad[input < 0] *= .01
        return output_grad 
```


```python
class LeakyReLU(nn.Module):
    def __init__(self):
        super().__init__()
        ''
        
        
    def forward(self, input):
        output = Leaky_ReLU_layer.apply(input)
        return output
    
```


```python
# instantiate models
tanh_model = NeuralNet(activation = Tanh()).to(device)
leaky_model = NeuralNet(activation = LeakyReLU()).to(device)

model = {'tanh': tanh_model,
        'leaky_relu': leaky_model}


optimizer = {'tanh': optim.SGD(tanh_model.parameters(), lr = .01),
        'leaky_relu': optim.SGD(leaky_model.parameters(), lr = .01)}


```


```python
N_EPOCHS = 25

# track statistics
track_stats = {'activation': [],
               'epoch': [],
               'train_loss': [],
              'train_acc': [],
              'valid_loss':[],
              'valid_acc':[]}

track_stats = {'tanh': track_stats.copy(),
              'leaky_relu': track_stats.copy()} 


for activation in ['tanh','leaky_relu']:
    
    # re-initiate per activation
    best_valid_loss = float('inf')


    for epoch in range(N_EPOCHS):

        start_time = time.time()

        train_loss, train_acc = train(model[activation], train_dl, optimizer[activation], criterion)
        valid_loss, valid_acc = evaluate(model[activation], eval_dl, criterion)

        end_time = time.time()

        # record operations
        track_stats[activation]['activation'].append(activation)
        track_stats[activation]['epoch'].append(epoch + 1)
        track_stats[activation]['train_loss'].append(train_loss)
        track_stats[activation]['train_acc'].append(train_acc)
        track_stats[activation]['valid_loss'].append(valid_loss)
        track_stats[activation]['valid_acc'].append(valid_acc)



        epoch_mins, epoch_secs = epoch_time(start_time, end_time)

        # if this was our best performance, record model parameters
        if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            torch.save(model[activation].state_dict(), 'best_'+activation+'_params.pt')

        # print out stats
        print('-'*75)
        print(f'Activation: {activation}')
        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')
        print(f'\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')
        print(f'\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')
        
        # if this is the last loop, save statistics
        if epoch + 1 == N_EPOCHS:
            torch.save(track_stats[activation],activation + '_stats.pt')
        


```

    ---------------------------------------------------------------------------
    Activation: tanh
    Epoch: 01 | Epoch Time: 0m 23s
    	Train Loss: 2.135 | Train Acc: 25.89%
    	 Val. Loss: 2.144 |  Val. Acc: 16.62%
    ---------------------------------------------------------------------------
    Activation: tanh
    Epoch: 02 | Epoch Time: 0m 38s
    	Train Loss: 1.789 | Train Acc: 43.35%
    	 Val. Loss: 2.489 |  Val. Acc: 15.23%
    ---------------------------------------------------------------------------
    Activation: tanh
    Epoch: 03 | Epoch Time: 0m 39s
    	Train Loss: 1.603 | Train Acc: 49.43%
    	 Val. Loss: 3.176 |  Val. Acc: 16.60%
    ---------------------------------------------------------------------------
    Activation: tanh
    Epoch: 04 | Epoch Time: 0m 39s
    	Train Loss: 1.510 | Train Acc: 51.15%
    	 Val. Loss: 3.610 |  Val. Acc: 16.60%
    ---------------------------------------------------------------------------
    Activation: tanh
    Epoch: 05 | Epoch Time: 0m 33s
    	Train Loss: 1.454 | Train Acc: 52.14%
    	 Val. Loss: 3.997 |  Val. Acc: 17.10%
    ---------------------------------------------------------------------------
    Activation: leaky_relu
    Epoch: 01 | Epoch Time: 0m 17s
    	Train Loss: 2.229 | Train Acc: 19.80%
    	 Val. Loss: 2.163 |  Val. Acc: 14.59%
    ---------------------------------------------------------------------------
    Activation: leaky_relu
    Epoch: 02 | Epoch Time: 0m 17s
    	Train Loss: 1.891 | Train Acc: 35.63%
    	 Val. Loss: 2.367 |  Val. Acc: 17.35%
    ---------------------------------------------------------------------------
    Activation: leaky_relu
    Epoch: 03 | Epoch Time: 0m 17s
    	Train Loss: 1.675 | Train Acc: 45.36%
    	 Val. Loss: 2.842 |  Val. Acc: 15.40%
    ---------------------------------------------------------------------------
    Activation: leaky_relu
    Epoch: 04 | Epoch Time: 0m 17s
    	Train Loss: 1.546 | Train Acc: 50.31%
    	 Val. Loss: 3.885 |  Val. Acc: 17.01%
    ---------------------------------------------------------------------------
    Activation: leaky_relu
    Epoch: 05 | Epoch Time: 0m 17s
    	Train Loss: 1.461 | Train Acc: 51.76%
    	 Val. Loss: 3.837 |  Val. Acc: 17.00%


```python
import pandas as pd

relu_stats = pd.DataFrame(torch.load('ReLU_stats.pt'))
tanh_stats = pd.DataFrame(track_stats['tanh'])
leaky_stats = pd.DataFrame(track_stats['leaky_relu'])
all_stats = pd.concat([relu_stats, tanh_stats, leaky_stats])
all_stats.head()
```


```python
data = []

for row in all_stats.iterrows():
    data.append(row[1].to_dict())        
import hiplot as hip
hip.Experiment.from_iterable(data).display(force_full_width = True)
```

<iframe src="/assets/html/relu_tanh_hiplot_graph.html" width="100%" height="500px"></iframe>

There are many insights our graph sheds light to:

* tanh and leaky ReLU attained the best validation accuracies 
* ReLU reached its best accuracy early on the process at epoch 3 while tanh and leaky ReLU reached their best performance along epochs 16 to 25. 

Just the above two insights let us know that ReLU very quickly overfits our data while tanh and leaky ReLU take more time, which is a desirable property.

Overall, if we had to choose one activation operation, we would go with tanh as it received the top two best validation accuracies.

# Conclusion

Activation functions are fundamental to any DL architecture and understanding their theory along with their implementation will make one more "aware" of the care and importance that goes on choosing the right activation function. Hopefully, it may also bring you to create your own, never before seen, activation function.

That is it for this tutorial. Thank you guys!





# Where to Next?

**Logistic Regression Tutorial:**
https://nbviewer.jupyter.org/github/Erick7451/DL-with-PyTorch/blob/master/Jupyter_Notebooks/logistic_regression.ipynb
